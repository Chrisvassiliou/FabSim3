default:
  # General default values for all machines live in the default dictionary.
  # Machine-specific values override (replace) these.
  # User-specific values in machines_user.yml override machine-specific values.

  # Default Modules to load or unload on remote, grouped by scriptname (or 'all' if to be used by all scripts).
  # e.g. ['load cmake'].
  modules: 
    all: []
    dummy: []
    
  # Allows to load or unload default modules on remote via interactive connection
  module_load_at_connect: true
  
  # Default number of cores per job.
  cores: 16
  
  # Commands to execute remotely as part of remote run jobs, prior to execution of the main code.
  run_prefix_commands: []
  
  # Templates to use to find space to build codes and execute workflows remotely.
  # All user- and generally- defined values in these config dictionaries will be interpolated via $foo
  home_path_template: "/home/$username"
  
  # Name for the Project FabSim3 folder
  fabric_dir: "FabSim3"
  
  # Path to runtime accessible filesystem (default is same as build time) - OBSOLETE?
  runtime_path_template: "$home_path"
  
  # Default number of cores per (supercomputer) node.
  corespernode: 1
  
  # Default number of cores per replica in an ensemble. - OBSOLETE?
  cores_per_replica: "${cores}"
  
  # Default command to use to launch remote jobs
  # e.g. qsub, empty string to launch interactively (e.g. on localhost, a VM or a machine without scheduler).
  job_dispatch: ''
  
  # Path to build filesystem folder
  # All user-defined values in these config dictionaries will be interpolated via $foo
  remote_path_template: "$home_path/$fabric_dir"
  
  # Path to run filesystem folder
  work_path_template: "$runtime_path/$fabric_dir"
  
  # Default batch job header template.
  # These templates are located in the templates/ subdirectory.
  batch_header: no_batch
  
  # Default command used to launch jobs on the nodes of a specific machine.
  run_command: "mpirun -np $cores"
  
  # Default location of the temporary file path - OBSOLETE?
  temp_path_template: "/tmp"
  
  # Default naming scheme used to label FabSim3 jobs.
  job_name_template: '${config}_${machine_name}_${cores}'
  
  # Flag to bypass paramiko and do direct remote SSH commands.
  manual_ssh: false
  
  # Flag to use GSISSH instead of SSH.
  manual_gsissh: false
  
  # Default location for templates.
  local_templates_path: ["$localroot/deploy/templates"]

  # Default command syntax for getting the job status.
  stat: "qstat -u $username"
  
  # Default command for cancelling jobs.
  cancel_job_command: "qdel"
  
  # Default arguments to LAMMPS - OBSOLETE?
  lammps_args: ""
  
  # Default location for blackbox scripts. - OBSOLETE?
  local_blackbox_path: ["$localroot/blackbox"]
  # Default location for Python scripts. - OBSOLETE?
  local_python_path: ["$localroot/python"]
  
  # Default location for config_files.
  local_config_file_path: ["$localroot/config_files"]


cartesius:
  max_job_name_chars: 15
  job_dispatch: "sbatch"
  stat: "squeue -u $username"
  run_command: "mpiexec -n $cores"
  batch_header: slurm-cartesius
  remote: "cartesius.surfsara.nl"
  home_path_template: "/home/$username"
  runtime_path_template: "/scratch-local/$username"
  modules:
    all: []
    gromacs: ["load gromacs"]
  temp_path_template: "$work_path/tmp"
  queue: "normal"
  python_build: "python2.7"
  corespernode: 24

prometheus:
  max_job_name_chars: 15
  job_dispatch: "sbatch"
  stat: "squeue -u $username"
  run_command: "mpiexec -n $cores"
  batch_header: slurm-prometheus
  remote: "prometheus.cyfronet.pl"
  home_path_template: "/net/people/$username"
  runtime_path_template: "/net/scratch/people/$username"
  modules: 
    all: ["load apps/lammps"]
  temp_path_template: "$work_path/tmp"
  queue: "plgrid"
  python_build: "python2.7"
  corespernode: 24


supermuc1_thin:
  max_job_name_chars: 15
  job_dispatch: "llsubmit"
  run_command: "poe"
  batch_header: ll-supermuc
  bac_ensemble_namd_script: bac-supermuc
  bac_ties_script: bac-ties-supermuc
  remote: "sb.supermuc.lrz.de"
  home_path_template: "/home/hpc/$project/$username"
  runtime_path_template: "/gpfs/work/$project/$username"
  modules: ["load namd/2.10"] #-25Sep11"]
  module_load_at_connect: false
  temp_path_template: "$work_path/tmp"
  queue: "standard"
  python_build: "lib64/python2.6"
  corespernode: 16
supermuc1_fat:
  max_job_name_chars: 15
  job_dispatch: "llsubmit"
  run_command: "poe"
  batch_header: ll-supermuc
  bac_ensemble_namd_script: bac-supermuc
  bac_ties_script: bac-ties-supermuc
  remote: "wm.supermuc.lrz.de"
  home_path_template: "/home/hpc/$project/$username"
  runtime_path_template: "/gpfs/work/$project/$username"
  modules: ["load namd/2.10"] #-25Sep11"]
  module_load_at_connect: false
  temp_path_template: "$work_path/tmp"
  queue: "standard"
  python_build: "lib64/python2.6"
  corespernode: 40
supermuc2:
  max_job_name_chars: 15
  job_dispatch: "llsubmit"
  run_command: "poe"
  batch_header: ll-supermuc
  bac_ensemble_namd_script: bac-supermuc
  bac_ties_script: bac-ties-supermuc
  remote: "hw.supermuc.lrz.de"
  home_path_template: "/home/hpc/$project/$username"
  runtime_path_template: "/gpfs/work/$project/$username"
  modules: ["load namd/2.10"] #-25Sep11"]
  module_load_at_connect: false
  temp_path_template: "$work_path/tmp"
  queue: "standard"
  python_build: "lib64/python2.6"
  corespernode: 28
archer:
  max_job_name_chars: 15
  job_dispatch: "qsub"
  run_command: "aprun -n $cores"
  run_ensemble_command: "aprun -n $cores_per_replica" # Required for ESMACS runs!
  run_ensemble_command_ties: "aprun -n $cores_per_replica_per_lambda" # Required for TIES runs!
  batch_header: pbs-archer
  bac_ensemble_namd_script: bac-archer
  bac_ensemble_nmode_script: bac-archer-nmode
  bac_ensemble_nm_remote_script: bac-archer-nm-remote
  bac_ties_script: bac-ties-archer-v2
  no_ssh: true # Hector doesn't allow outgoing ssh sessions.
  remote: "login.archer.ac.uk"
  # On hector, *all files* which are needed at runtime, must be on the /work filesystem, so we must make the install location be on the /work filesystem
  home_path_template: "/home/$project/$project/$username"
  runtime_path_template: "/work/$project/$project/$username"
  stat: "qstat -u $username"
  modules: 
    all: []
    lammps: ["load lammps/lammps-28Jun14"]
    dummy_lammps: ["load lammps/lammps-28Jun14"]
    namd: ["load namd"]
    gromacs: ["load gromacs"]
  temp_path_template: "$work_path/tmp"
  queue: "standard"
  python_build: "lib64/python2.6"
  pwd: "export PBS_O_WORKDIR=$(readlink -f $$PBS_O_WORKDIR)"
  corespernode: 24
bluejoule:
  job_dispatch: "llsubmit"
  run_command: "runjob --env-all -p $corespernode -n $cores :"
  run_ensemble_command: "runjob -n $cores_per_replica"
  batch_header: ll
  remote: "login.joule.hartree.stfc.ac.uk"
  home_path_template: "/gpfs/home/$project/$groupname/$username"
  runtime_path_template: "/gpfs/home/$project/$groupname/$username"
  modules: ["load namd/2.9"]
  queue: "standard"
  corespernode: 16
  pwd: "#"
  stat: "llq -u $username"
bluewonder1:
  job_dispatch: "bsub <"
  run_command: "mpiexec.hydra -n $cores"
  run_ensemble_command: "mpiexec.hydra -n $cores_per_replica"
  batch_header: lsf1
  bac_ensemble_namd_script: bac-wonder-namd
  bac_ensemble_nmode_script: bac-wonder1-nmode
#  bac_ensemble_nmode_script: bac-wonder1-nmode-test
  bac_ensemble_nm_remote_script: bac-wonder1-nm-remote
  remote: "phase1.wonder.hartree.stfc.ac.uk"
  home_path_template: "/gpfs/home/$project/$groupname/$username"
  runtime_path_template: "/gpfs/home/$project/$groupname/$username"
  modules: ["load namd/2.9"]
  queue: "standard"
  corespernode: 16
  pwd: "#"
  stat: "bjobs -W $username"
bluewonder2:
  job_dispatch: "bsub <"
  run_command: "mpiexec.hydra -n $cores"
  run_ensemble_command: "mpiexec.hydra -n $cores_per_replica"
  batch_header: lsf2
  bac_ensemble_namd_script: bac-wonder-namd
  bac_ensemble_nmode_script: bac-wonder2-nmode
#  bac_ensemble_nmode_script: bac-wonder2-nmode-test
  bac_ensemble_nm_remote_script: bac-wonder2-nm-remote
  remote: "phase2.wonder.hartree.stfc.ac.uk"
  home_path_template: "/gpfs/stfc/local/$project/$groupname/$username"
  runtime_path_template: "/gpfs/stfc/local/$project/$groupname/$username"
  modules: ["load namd/2.9"]
  queue: "standard"
  corespernode: 24
  pwd: "#"
oppenheimer:
  remote: "oppenheimer.chem.ucl.ac.uk"
  run_command: "/opt/openmpi/gfortran/1.4.3/bin/mpirun -np $cores"
  batch_header: sge_oppenheimer
  no_hg: true
  job_dispatch: "qsub"
  python_build: "lib/python2.6"
localhost:
  remote: "localhost"
  python_build: "lib/python2.7"
  home_path_template: "/home/$username"
  # The setting below can be useful for debugging if you run into problems.
  # manual_ssh: true
mavrino:
  max_job_name_chars: 15
  # on mavrino, job names can't start with a digit, so we'll just prefix the bloody thing with a 'p' to prevent user annoyance.
  job_name_template: '${config}_${machine_name}_${cores}'
  job_dispatch: "qsub"
  run_command: "/opt/mpich2/gnu/bin/mpirun -np $cores"
  remote: "mavrino"
  home_path_template: "/home/$username"
  runtime_path_template: "/home/$username"
  temp_path_template: "$work_path/tmp"
  corespernode: 8
  batch_header: sge
  queue: "parallel2" #"mapper"

qcg:
  remote: "eagle.man.poznan.pl" 

  # PilotJob attributes
  PJ_header: qcg-PJ-header
  PJ_PYheader: qcg-PJ.py
  #PJ_header: no_batch
  PJ_size : "3"
  PJ_wall_time : "PT10M"

  # For QCG, env.remote is the machine where we will store and retrieve the input/output files.
  # This is NOT the machine where we run batch scripts; this is done using the QCG-Client instead.  
  home_path_template: "/home/plgrid/$username"
  manual_ssh: false
  dispatch_jobs_on_localhost: false #TODO: implement this in fab/base in the def job() function, so that job dispatch is done locally.
  run_command: "mpirun -n $cores"

  batch_header: qcg-eagle
  qcg_MODULEPATH : "/home/plgrid-groups/plggvecma/.qcg-modules"
  eagle_modules: "vecma/apps/flee"
  queue: "plgrid"

  budget: "vecma2019"
  corespernode: 2

  # stat : return report for all submitted jobs
  # it should be set in the way that only return jobID, job status, and host in the output (without column header)
  # for example: the template output should be similar to :
  #           ...   ...       ...
  #           job1  FINISHED  eagle
  #           job2  FAILED    eagle
  #           job2  CANCELED  eagle  
  #           ...   ...       ...  
  stat: 'qcg-list -Q -s all -F "%-22I %-16S %-8H" | awk "{if(NR>2)print}"'

  # job_dispatch : the command to submit the task to be processed by remote machine
  job_dispatch: "qcg-sub"

  # cancel_job_command : cancel submitted task based on input $jobID 
  # the $jobID variable will be set during the execution, keep the format as '$jobID' in the parameters list
  cancel_job_command: 'qcg-cancel $jobID'

  # pilot job manager installation
  virtual_env_path: "/home/plgrid/$username/MyVirtualEnv"
  app_repository: "/home/plgrid/$username/VECMA/App_repo"
  job_name: "VECMA"

eagle:
  run_command: "mpirun -n $cores"
  batch_header: slurm-eagle
  eagle_MODULEPATH : "/home/plgrid-groups/plggvecma/.qcg-modules"
  eagle_modules: "vecma/apps/fleev2"
  remote: "eagle.man.poznan.pl" 
  home_path_template: "/home/plgrid/$username"
  queue: "plgrid"
  budget: "vecma2019"
  corespernode: 2
  manual_ssh: false

  # PilotJob attributes
  PJ_header: eagle-PJ-header
  PJ_PYheader: eagle-PJ.py
  #PJ_header: no_batch
  PJ_size : "3"
  # wall time in format MINUTES:SECONDS
  PJ_wall_time : "10:00"

  budget: "vecma2019"
  corespernode: 2


  #stat: "sacct -o jobid,state,cluster -X -n"
  stat: "sacct --format=jobid,state,cluster --allocations --noheader"
  job_dispatch: "sbatch"
  cancel_job_command: 'scancel $jobID'
  #job_info_command : 'sacct -X -j $jobID'  
  job_info_command : 'sacct --allocations --job $jobID'
  
  # pilot job manager installation
  virtual_env_path: "/home/plgrid/$username/MyVirtualEnv"
  app_repository: "/home/plgrid/$username/VECMA/App_repo"
  job_name: "VECMA"
  
  #max_job_name_chars: 15
  #modules:
  #  all: []  
  #runtime_path_template: "/tmp/lustre_shared/$username"
  #temp_path_template: "$work_path/tmp"  
