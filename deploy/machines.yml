#
# Copyright (C) University College London, 2007-2012, all rights reserved.
#
# This file is part of HemeLB and is CONFIDENTIAL. You may not work
# with, install, use, duplicate, modify, redistribute or share this
# file, or any part thereof, other than as allowed by any agreement
# specifically made by you with University College London.
#

default:
  # General default values for all machines live in the default dictionary.
  # Machine-specific overrides replace these.
  # Mercurial server to find HemeLB
  hg: "ssh://hg@entropy.chem.ucl.ac.uk"
  # Default Modules to load or unload on remote, via module 'foo'
  # e.g. ['load cmake'].
  modules: 
    all: []
    dummy: []
  # Allows to load or unload default modules on remote via interactive connection
  module_load_at_connect: true
  cores: 16
  # Commands to execute as part of remote run jobs.
  run_prefix_commands: []
  # Templates to use to find space to checkout and build HemeLB on the remote.
  # All user- and generally- defined values in these config dictionaries will be interpolated via $foo
  home_path_template: "/home/$username"
  # Name for the Project Fabric folder
  fabric_dir: "FabSim3"
  # Path to runtime accessible filesystem (default is same as build time)
  runtime_path_template: "$home_path"
  corespernode: 1
  cores_per_replica: "${cores}"
  # Default options used in the CMake configure step.
  # Command to use to launch remote jobs
  # e.g. qsub, empty string to launch interactively.
  job_dispatch: ''
  # Path to build filesystem folder
  # All user-defined values in these config dictionaries will be interpolated via $foo
  remote_path_template: "$home_path/$fabric_dir"
  # Path to run filesystem folder
  work_path_template: "$runtime_path/$fabric_dir"
  # Normally, the regression test can be run directly out of diffTest, but sometimes needs to be copied to a runtime file area.
  batch_header: no_batch
  run_command: "mpirun -np $cores"
  temp_path_template: "/tmp"
  job_name_template: '${config}_${machine_name}_${cores}'
  manual_ssh: false
  manual_gsissh: false
  # This variable encodes the default location for templates.
  local_templates_path: ["$localroot/deploy/templates"]
  stat: qstat
  cancel_job_command: "qdel"
  lammps_args: ""
  # This variable encodes the default location for blackbox scripts.
  local_blackbox_path: ["$localroot/blackbox"]
  # This variable encodes the default location for Python scripts.
  local_python_path: ["$localroot/python"]
  # This variable encodes the default location for config_files.
  local_config_file_path: ["$localroot/config_files"]

prometheus:
  max_job_name_chars: 15
  job_dispatch: "sbatch"
  stat: "squeue"
  run_command: "mpiexec -n $cores"
  batch_header: slurm-prometheus
  remote: "prometheus.cyfronet.pl"
  home_path_template: "/net/people/$username"
  runtime_path_template: "/net/scratch/people/$username"
  modules: ["load apps/lammps"]
  temp_path_template: "$work_path/tmp"
  queue: "plgrid"
  python_build: "python2.7"
  corespernode: 24

eagle:
  max_job_name_chars: 15
  job_dispatch: "sbatch"
  stat: "squeue --noheader"
  run_command: "mpirun -n $cores"
  batch_header: slurm-eagle
  remote: "eagle.man.poznan.pl"
  home_path_template: "/home/plgrid/$username"
  runtime_path_template: "/tmp/lustre_shared/$username"
  temp_path_template: "$work_path/tmp"
  modules: ["load plgrid/tools/openmpi/1.10.2-1_gcc482"]
  queue: "plgrid"
  budget: "compatpsnc2"
  corespernode: 28

supermuc1_thin:
  max_job_name_chars: 15
  job_dispatch: "llsubmit"
  run_command: "poe"
  batch_header: ll-supermuc
  bac_ensemble_namd_script: bac-supermuc
  bac_ties_script: bac-ties-supermuc
  remote: "sb.supermuc.lrz.de"
  home_path_template: "/home/hpc/$project/$username"
  runtime_path_template: "/gpfs/work/$project/$username"
  modules: ["load namd/2.10"] #-25Sep11"]
  module_load_at_connect: false
  temp_path_template: "$work_path/tmp"
  queue: "standard"
  python_build: "lib64/python2.6"
  corespernode: 16
supermuc1_fat:
  max_job_name_chars: 15
  job_dispatch: "llsubmit"
  run_command: "poe"
  batch_header: ll-supermuc
  bac_ensemble_namd_script: bac-supermuc
  bac_ties_script: bac-ties-supermuc
  remote: "wm.supermuc.lrz.de"
  home_path_template: "/home/hpc/$project/$username"
  runtime_path_template: "/gpfs/work/$project/$username"
  modules: ["load namd/2.10"] #-25Sep11"]
  module_load_at_connect: false
  temp_path_template: "$work_path/tmp"
  queue: "standard"
  python_build: "lib64/python2.6"
  corespernode: 40
supermuc2:
  max_job_name_chars: 15
  job_dispatch: "llsubmit"
  run_command: "poe"
  batch_header: ll-supermuc
  bac_ensemble_namd_script: bac-supermuc
  bac_ties_script: bac-ties-supermuc
  remote: "hw.supermuc.lrz.de"
  home_path_template: "/home/hpc/$project/$username"
  runtime_path_template: "/gpfs/work/$project/$username"
  modules: ["load namd/2.10"] #-25Sep11"]
  module_load_at_connect: false
  temp_path_template: "$work_path/tmp"
  queue: "standard"
  python_build: "lib64/python2.6"
  corespernode: 28
archer:
  max_job_name_chars: 15
  job_dispatch: "qsub"
  run_command: "aprun -n $cores"
  run_ensemble_command: "aprun -n $cores_per_replica" # Required for ESMACS runs!
  run_ensemble_command_ties: "aprun -n $cores_per_replica_per_lambda" # Required for TIES runs!
  batch_header: pbs-archer
  bac_ensemble_namd_script: bac-archer
  bac_ensemble_nmode_script: bac-archer-nmode
  bac_ensemble_nm_remote_script: bac-archer-nm-remote
  bac_ties_script: bac-ties-archer-v2
  no_ssh: true # Hector doesn't allow outgoing ssh sessions.
  remote: "login.archer.ac.uk"
  # On hector, *all files* which are needed at runtime, must be on the /work filesystem, so we must make the install location be on the /work filesystem
  home_path_template: "/home/$project/$project/$username"
  runtime_path_template: "/work/$project/$project/$username"
  modules: 
    all: []
    lammps: ["load lammps/lammps-28Jun14"]
    dummy_lammps: ["load lammps/lammps-28Jun14"]
    namd: ["load namd"]
    gromacs: ["load gromacs"]
  temp_path_template: "$work_path/tmp"
  queue: "standard"
  python_build: "lib64/python2.6"
  pwd: "export PBS_O_WORKDIR=$(readlink -f $$PBS_O_WORKDIR)"
  corespernode: 24
bluejoule:
  job_dispatch: "llsubmit"
  run_command: "runjob --env-all -p $corespernode -n $cores :"
  run_ensemble_command: "runjob -n $cores_per_replica"
  batch_header: ll
  remote: "login.joule.hartree.stfc.ac.uk"
  home_path_template: "/gpfs/home/$project/$groupname/$username"
  runtime_path_template: "/gpfs/home/$project/$groupname/$username"
  modules: ["load namd/2.9"]
  queue: "standard"
  corespernode: 16
  pwd: "#"
  stat: "llq -u"
  stat_postfix: = '| grep -q -v "There is currently no job status to report"'
bluewonder1:
  job_dispatch: "bsub <"
  run_command: "mpiexec.hydra -n $cores"
  run_ensemble_command: "mpiexec.hydra -n $cores_per_replica"
  batch_header: lsf1
  bac_ensemble_namd_script: bac-wonder-namd
  bac_ensemble_nmode_script: bac-wonder1-nmode
#  bac_ensemble_nmode_script: bac-wonder1-nmode-test
  bac_ensemble_nm_remote_script: bac-wonder1-nm-remote
  remote: "phase1.wonder.hartree.stfc.ac.uk"
  home_path_template: "/gpfs/home/$project/$groupname/$username"
  runtime_path_template: "/gpfs/home/$project/$groupname/$username"
  modules: ["load namd/2.9"]
  queue: "standard"
  corespernode: 16
  pwd: "#"
  stat: "bjobs -W"
bluewonder2:
  job_dispatch: "bsub <"
  run_command: "mpiexec.hydra -n $cores"
  run_ensemble_command: "mpiexec.hydra -n $cores_per_replica"
  batch_header: lsf2
  bac_ensemble_namd_script: bac-wonder-namd
  bac_ensemble_nmode_script: bac-wonder2-nmode
#  bac_ensemble_nmode_script: bac-wonder2-nmode-test
  bac_ensemble_nm_remote_script: bac-wonder2-nm-remote
  remote: "phase2.wonder.hartree.stfc.ac.uk"
  home_path_template: "/gpfs/stfc/local/$project/$groupname/$username"
  runtime_path_template: "/gpfs/stfc/local/$project/$groupname/$username"
  modules: ["load namd/2.9"]
  queue: "standard"
  corespernode: 24
  pwd: "#"
oppenheimer:
  remote: "oppenheimer.chem.ucl.ac.uk"
  run_command: "/opt/openmpi/gfortran/1.4.3/bin/mpirun -np $cores"
  batch_header: sge_oppenheimer
  no_hg: true
  job_dispatch: "qsub"
  python_build: "lib/python2.6"
localhost:
  remote: "localhost"
  python_build: "lib/python2.7"
  home_path_template: "/home/$username"
  # The setting below can be useful for debugging if you run into problems.
  # manual_ssh: true
mavrino:
  max_job_name_chars: 15
  # on mavrino, job names can't start with a digit, so we'll just prefix the bloody thing with a 'p' to prevent user annoyance.
  job_name_template: '${config}_${machine_name}_${cores}'
  job_dispatch: "qsub"
  run_command: "/opt/mpich2/gnu/bin/mpirun -np $cores"
  remote: "mavrino"
  home_path_template: "/home/$username"
  runtime_path_template: "/home/$username"
  temp_path_template: "$work_path/tmp"
  corespernode: 8
  batch_header: sge
  queue: "parallel2" #"mapper"
