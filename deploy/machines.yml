#
# Copyright (C) University College London, 2007-2012, all rights reserved.
#
# This file is part of HemeLB and is CONFIDENTIAL. You may not work
# with, install, use, duplicate, modify, redistribute or share this
# file, or any part thereof, other than as allowed by any agreement
# specifically made by you with University College London.
#

default:
  # General default values for all machines live in the default dictionary.
  # Machine-specific overrides replace these.
  # Mercurial server to find HemeLB
  hg: "ssh://hg@entropy.chem.ucl.ac.uk"
  # Default Modules to load or unload on remote, via module 'foo'
  # e.g. ['load cmake'].
  modules: 
    all: []
    dummy: []
  # Allows to load or unload default modules on remote via interactive connection
  module_load_at_connect: true
  cores: 16
  # Commands to execute as part of remote run jobs.
  run_prefix_commands: []
  # Templates to use to find space to checkout and build HemeLB on the remote.
  # All user- and generally- defined values in these config dictionaries will be interpolated via $foo
  home_path_template: "/home/$username"
  # Name for the Project Fabric folder
  fabric_dir: "FabSim3"
  # Path to runtime accessible filesystem (default is same as build time)
  runtime_path_template: "$home_path"
  corespernode: 1
  cores_per_replica: "${cores}"
  # Default options used in the CMake configure step.
  # Command to use to launch remote jobs
  # e.g. qsub, empty string to launch interactively.
  job_dispatch: ''
  # Path to build filesystem folder
  # All user-defined values in these config dictionaries will be interpolated via $foo
  remote_path_template: "$home_path/$fabric_dir"
  # Path to run filesystem folder
  work_path_template: "$runtime_path/$fabric_dir"
  # Normally, the regression test can be run directly out of diffTest, but sometimes needs to be copied to a runtime file area.
  batch_header: no_batch
  run_command: "mpirun -np $cores"
  temp_path_template: "/tmp"
  job_name_template: '${config}_${machine_name}_${cores}'
  manual_ssh: false
  manual_gsissh: false
  # This variable encodes the default location for templates.
  local_templates_path: ["$localroot/deploy/templates"]
  stat: qstat
  cancel_job_command: "qdel"
  lammps_args: ""
  # This variable encodes the default location for blackbox scripts.
  local_blackbox_path: ["$localroot/blackbox"]
  # This variable encodes the default location for Python scripts.
  local_python_path: ["$localroot/python"]
  # This variable encodes the default location for config_files.
  local_config_file_path: ["$localroot/config_files"]


cartesius:
  max_job_name_chars: 15
  job_dispatch: "sbatch"
  stat: "squeue"
  run_command: "mpiexec -n $cores"
  batch_header: slurm-cartesius
  remote: "cartesius.surfsara.nl"
  home_path_template: "/home/$username"
  runtime_path_template: "/scratch-local/$username"
  modules:
    all: []
    gromacs: ["load gromacs"]
  temp_path_template: "$work_path/tmp"
  queue: "normal"
  python_build: "python2.7"
  corespernode: 24

prometheus:
  max_job_name_chars: 15
  job_dispatch: "sbatch"
  stat: "squeue"
  run_command: "mpiexec -n $cores"
  batch_header: slurm-prometheus
  remote: "prometheus.cyfronet.pl"
  home_path_template: "/net/people/$username"
  runtime_path_template: "/net/scratch/people/$username"
  modules: 
    all: ["load apps/lammps"]
  temp_path_template: "$work_path/tmp"
  queue: "plgrid"
  python_build: "python2.7"
  corespernode: 24


supermuc1_thin:
  max_job_name_chars: 15
  job_dispatch: "llsubmit"
  run_command: "poe"
  batch_header: ll-supermuc
  bac_ensemble_namd_script: bac-supermuc
  bac_ties_script: bac-ties-supermuc
  remote: "sb.supermuc.lrz.de"
  home_path_template: "/home/hpc/$project/$username"
  runtime_path_template: "/gpfs/work/$project/$username"
  modules: ["load namd/2.10"] #-25Sep11"]
  module_load_at_connect: false
  temp_path_template: "$work_path/tmp"
  queue: "standard"
  python_build: "lib64/python2.6"
  corespernode: 16
supermuc1_fat:
  max_job_name_chars: 15
  job_dispatch: "llsubmit"
  run_command: "poe"
  batch_header: ll-supermuc
  bac_ensemble_namd_script: bac-supermuc
  bac_ties_script: bac-ties-supermuc
  remote: "wm.supermuc.lrz.de"
  home_path_template: "/home/hpc/$project/$username"
  runtime_path_template: "/gpfs/work/$project/$username"
  modules: ["load namd/2.10"] #-25Sep11"]
  module_load_at_connect: false
  temp_path_template: "$work_path/tmp"
  queue: "standard"
  python_build: "lib64/python2.6"
  corespernode: 40
supermuc2:
  max_job_name_chars: 15
  job_dispatch: "llsubmit"
  run_command: "poe"
  batch_header: ll-supermuc
  bac_ensemble_namd_script: bac-supermuc
  bac_ties_script: bac-ties-supermuc
  remote: "hw.supermuc.lrz.de"
  home_path_template: "/home/hpc/$project/$username"
  runtime_path_template: "/gpfs/work/$project/$username"
  modules: ["load namd/2.10"] #-25Sep11"]
  module_load_at_connect: false
  temp_path_template: "$work_path/tmp"
  queue: "standard"
  python_build: "lib64/python2.6"
  corespernode: 28
archer:
  max_job_name_chars: 15
  job_dispatch: "qsub"
  run_command: "aprun -n $cores"
  run_ensemble_command: "aprun -n $cores_per_replica" # Required for ESMACS runs!
  run_ensemble_command_ties: "aprun -n $cores_per_replica_per_lambda" # Required for TIES runs!
  batch_header: pbs-archer
  bac_ensemble_namd_script: bac-archer
  bac_ensemble_nmode_script: bac-archer-nmode
  bac_ensemble_nm_remote_script: bac-archer-nm-remote
  bac_ties_script: bac-ties-archer-v2
  no_ssh: true # Hector doesn't allow outgoing ssh sessions.
  remote: "login.archer.ac.uk"
  # On hector, *all files* which are needed at runtime, must be on the /work filesystem, so we must make the install location be on the /work filesystem
  home_path_template: "/home/$project/$project/$username"
  runtime_path_template: "/work/$project/$project/$username"
  stat: "qstat -u $username"
  modules: 
    all: []
    lammps: ["load lammps/lammps-28Jun14"]
    dummy_lammps: ["load lammps/lammps-28Jun14"]
    namd: ["load namd"]
    gromacs: ["load gromacs"]
  temp_path_template: "$work_path/tmp"
  queue: "standard"
  python_build: "lib64/python2.6"
  pwd: "export PBS_O_WORKDIR=$(readlink -f $$PBS_O_WORKDIR)"
  corespernode: 24
bluejoule:
  job_dispatch: "llsubmit"
  run_command: "runjob --env-all -p $corespernode -n $cores :"
  run_ensemble_command: "runjob -n $cores_per_replica"
  batch_header: ll
  remote: "login.joule.hartree.stfc.ac.uk"
  home_path_template: "/gpfs/home/$project/$groupname/$username"
  runtime_path_template: "/gpfs/home/$project/$groupname/$username"
  modules: ["load namd/2.9"]
  queue: "standard"
  corespernode: 16
  pwd: "#"
  stat: "llq -u"
  stat_postfix: = '| grep -q -v "There is currently no job status to report"'
bluewonder1:
  job_dispatch: "bsub <"
  run_command: "mpiexec.hydra -n $cores"
  run_ensemble_command: "mpiexec.hydra -n $cores_per_replica"
  batch_header: lsf1
  bac_ensemble_namd_script: bac-wonder-namd
  bac_ensemble_nmode_script: bac-wonder1-nmode
#  bac_ensemble_nmode_script: bac-wonder1-nmode-test
  bac_ensemble_nm_remote_script: bac-wonder1-nm-remote
  remote: "phase1.wonder.hartree.stfc.ac.uk"
  home_path_template: "/gpfs/home/$project/$groupname/$username"
  runtime_path_template: "/gpfs/home/$project/$groupname/$username"
  modules: ["load namd/2.9"]
  queue: "standard"
  corespernode: 16
  pwd: "#"
  stat: "bjobs -W"
bluewonder2:
  job_dispatch: "bsub <"
  run_command: "mpiexec.hydra -n $cores"
  run_ensemble_command: "mpiexec.hydra -n $cores_per_replica"
  batch_header: lsf2
  bac_ensemble_namd_script: bac-wonder-namd
  bac_ensemble_nmode_script: bac-wonder2-nmode
#  bac_ensemble_nmode_script: bac-wonder2-nmode-test
  bac_ensemble_nm_remote_script: bac-wonder2-nm-remote
  remote: "phase2.wonder.hartree.stfc.ac.uk"
  home_path_template: "/gpfs/stfc/local/$project/$groupname/$username"
  runtime_path_template: "/gpfs/stfc/local/$project/$groupname/$username"
  modules: ["load namd/2.9"]
  queue: "standard"
  corespernode: 24
  pwd: "#"
oppenheimer:
  remote: "oppenheimer.chem.ucl.ac.uk"
  run_command: "/opt/openmpi/gfortran/1.4.3/bin/mpirun -np $cores"
  batch_header: sge_oppenheimer
  no_hg: true
  job_dispatch: "qsub"
  python_build: "lib/python2.6"
localhost:
  remote: "localhost"
  python_build: "lib/python2.7"
  home_path_template: "/home/$username"
  # The setting below can be useful for debugging if you run into problems.
  # manual_ssh: true
mavrino:
  max_job_name_chars: 15
  # on mavrino, job names can't start with a digit, so we'll just prefix the bloody thing with a 'p' to prevent user annoyance.
  job_name_template: '${config}_${machine_name}_${cores}'
  job_dispatch: "qsub"
  run_command: "/opt/mpich2/gnu/bin/mpirun -np $cores"
  remote: "mavrino"
  home_path_template: "/home/$username"
  runtime_path_template: "/home/$username"
  temp_path_template: "$work_path/tmp"
  corespernode: 8
  batch_header: sge
  queue: "parallel2" #"mapper"

qcg:
  remote: "eagle.man.poznan.pl" 

  # PilotJob attributes
  PJ_header: qcg-PJ-header
  PJ_PYheader: qcg-PJ.py
  #PJ_header: no_batch
  PJ_size : "3"
  PJ_wall_time : "PT10M"

  # For QCG, env.remote is the machine where we will store and retrieve the input/output files.
  # This is NOT the machine where we run batch scripts; this is done using the QCG-Client instead.  
  home_path_template: "/home/plgrid/$username/"
  manual_ssh: false
  dispatch_jobs_on_localhost: false #TODO: implement this in fab/base in the def job() function, so that job dispatch is done locally.
  run_command: "mpirun -n $cores"

  batch_header: qcg-eagle
  qcg_MODULEPATH : "/home/plgrid-groups/plggvecma/.qcg-modules"
  eagle_modules: "vecma/apps/flee"
  queue: "plgrid"

  budget: "vecma2019"
  corespernode: 2

  # stat : return report for all submitted jobs
  # it should be set in the way that only return jobID, job status, and host in the output (without column header)
  # for example: the template output should be similar to :
  #           ...   ...       ...
  #           job1  FINISHED  eagle
  #           job2  FAILED    eagle
  #           job2  CANCELED  eagle  
  #           ...   ...       ...  
  stat: 'qcg-list -Q -s all -F "%-22I %-16S %-8H" | awk "{if(NR>2)print}"'

  # job_dispatch : the command to submit the task to be processed by remote machine
  job_dispatch: "qcg-sub"

  # cancel_job_command : cancel submitted task based on input $jobID 
  # the $jobID variable will be set during the execution, keep the format as '$jobID' in the parameters list
  cancel_job_command: 'qcg-cancel $jobID'
      
eagle:
  run_command: "mpirun -n $cores"
  batch_header: slurm-eagle
  eagle_MODULEPATH : "/home/plgrid-groups/plggvecma/.qcg-modules"
  eagle_modules: "vecma/apps/flee"
  remote: "eagle.man.poznan.pl" 
  home_path_template: "/home/plgrid/$username"
  queue: "plgrid"
  budget: "vecma2019"
  corespernode: 2
  manual_ssh: false
  #stat: "sacct -o jobid,state,cluster -X -n"
  stat: "sacct --format=jobid,state,cluster --allocations --noheader"
  job_dispatch: "sbatch"
  cancel_job_command: 'scancel $jobID'
  #job_info_command : 'sacct -X -j $jobID'  
  job_info_command : 'sacct --allocations --job $jobID'
  

  #max_job_name_chars: 15
  #modules:
  #  all: []  
  #runtime_path_template: "/tmp/lustre_shared/$username"
  #temp_path_template: "$work_path/tmp"  
